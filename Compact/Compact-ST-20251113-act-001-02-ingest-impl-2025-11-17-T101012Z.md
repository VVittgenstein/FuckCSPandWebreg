### Confirmed implementation
- `scripts/fetch_soc_data.ts` is no longer a planner-only stub: after parsing the same CLI/config schema it now loads the configured SQLite DB, enforces optional clean-worktree safety, stages raw SOC responses under `data/staging/term/campus`, and executes real ingestion runs slice-by-slice.
- Ingestion uses the new normalizer (`scripts/soc_normalizer.ts`) to derive deterministic course/section/meeting hashes (SHA1 of sorted payloads), human-friendly fields (meeting modes, credits, delivery method), and reference-table metadata so SQLite upserts stay idempotent.
- Each `term+campus+subject` batch runs inside a better-sqlite3 transaction: reference rows (terms/campuses/subjects) are upserted first, then courses/sections are inserted/updated/deleted based on hash comparisons, and `section_meetings` are rebuilt only for sections whose payload changed. Status flips emit `section_status_events` with `courses.json` as the source.
- When `concurrency.maxOpenSectionsWorkers>0`, the runner now calls `openSections`, stages the payload, reconciles `sections.is_open/open_status` against the returned indexes, writes `open_section_snapshots`, and logs how many sections were toggled.
- Each slice produces `SubjectStats` plus elapsed time; the whole run emits JSON + text summaries under `logs/fetch_runs/summary_latest.*` so downstream monitoring can read insert/update/delete totals.

### Interface / behavior changes
- CLI flags and config schema stay the same, but the command now performs real network + DB work unless `--dry-run` or `safety.dryRun` is set. Operators must ensure the configured SQLite path exists (or will be created) and that migrations have already run.
- Enabling `safety.requireCleanWorktree` will abort execution if `git status --porcelain` is non-empty; disable it in the config for local experiments.
- Successful runs now mutate `data/courses.sqlite` and leave staged payloads + logs; CI/ops scripts consuming these files should expect them to exist.

### Self-test evidence
- `npm run data:fetch -- --config configs/fetch_pipeline.example.json --terms 12024 --campuses NB --subjects 198 --dry-run` (config validation without side effects).
- `npm run data:fetch -- --config configs/fetch_pipeline.dev.json --terms 12024 --campuses NB --subjects 198 --mode incremental` (using a copy of the sample config with `safety.requireCleanWorktree=false`): ingested 66 courses / 627 sections for term 12024, campus NB, and produced summary files under `logs/fetch_runs/`.

### Risks / TODOs
- Current execution processes subjects serially; the concurrency knobs are parsed but not yet tied to actual worker pools or throttling, so large runs may take longer than the config suggests.
- Only a single subject+campus combination was verified end-to-end; full-term coverage and incremental-to-full-init transitions still need validation.
- Operators must manage `data/staging` growth; no automatic cleanup was implemented yet.

## Code Review - ST-20251113-act-001-02-ingest-impl - 2025-11-17T10:18:20Z
scripts/soc_normalizer.ts
Comment on lines +208 to +209
  const openStatus = ensureString(raw.openStatusText ?? raw.openStatus ?? '').toUpperCase() || 'UNKNOWN';
  const isOpen = Boolean(raw.openStatus ?? openStatus.includes('OPEN'));
@chatgpt-codex-connector
chatgpt-codex-connector bot
1分钟前
P1 Badge Derive section openness from status value, not truthiness

Sections are flagged as open whenever raw.openStatus is present because isOpen is computed as Boolean(raw.openStatus ?? openStatus.includes('OPEN')). If the SOC payload sets openStatus to a non-empty string like "CLOSED", this expression still yields true, so closed sections will be stored and reported as open (affecting hashes, status events, and downstream open/closed counts). The check should inspect the status value, not just its presence.

## Code Review - ST-20251113-act-001-02-ingest-impl - 2025-11-17T10:22:57Z
Codex Review: Didn't find any major issues. Already looking forward to the next diff.
